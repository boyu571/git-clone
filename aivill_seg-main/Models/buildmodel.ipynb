{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [

    "# Build model for 2020 the ai competition\n",

    "- Reco model: pretrained model\n",
    "- U2PL model: data refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as transforms_f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''parser = argparse.ArgumentParser(description='Semi-supervised Segmentation with Perfect Labels')\n",
    "parser.add_argument('--mode', default=None, type=str)\n",
    "\n",
    "parser.add_argument('--gpu', default=0, type=int)\n",
    "parser.add_argument('--num_labels', default=15, type=int, help='number of labelled training data, set 0 to use all training data')\n",
    "parser.add_argument('--lr', default=2.5e-3, type=float)\n",
    "parser.add_argument('--weight_decay', default=5e-4, type=float)\n",
    "parser.add_argument('--dataset', default='cityscapes', type=str, help='pascal, cityscapes, sun')\n",
    "parser.add_argument('--apply_aug', default='cutout', type=str, help='apply semi-supervised method: cutout cutmix classmix')\n",
    "parser.add_argument('--id', default=1, type=int, help='number of repeated samples')\n",
    "parser.add_argument('--weak_threshold', default=0.7, type=float)\n",
    "parser.add_argument('--strong_threshold', default=0.97, type=float)\n",
    "parser.add_argument('--apply_reco', action='store_true')\n",
    "parser.add_argument('--num_negatives', default=512, type=int, help='number of negative keys')\n",
    "parser.add_argument('--num_queries', default=256, type=int, help='number of queries per segment per image')\n",
    "parser.add_argument('--temp', default=0.5, type=float)\n",
    "parser.add_argument('--output_dim', default=256, type=int, help='output dimension from representation head')\n",
    "parser.add_argument('--backbone', default='deeplabv3p', type=str, help='choose backbone: deeplabv3p, deeplabv2')\n",
    "parser.add_argument('--seed', default=0, type=int)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed=42)\n",
    "np.random.seed(seed=42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:{:d}\".format(args.gpu) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기, 가공, 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image index\n",
    "def img_idx(root, train=True, label_num = 5):\n",
    "    root = os.path.expanduser(root)\n",
    "    if train:\n",
    "        file_name = root + '/train_aug.txt'\n",
    "    else:\n",
    "        file_name = root + '/val.txt'\n",
    "    with open(file_name) as f:\n",
    "        idx_list = f.read().splitlines()\n",
    "\n",
    "    if train:\n",
    "        labeled_idx = []\n",
    "        save_idx = []\n",
    "        idx_list_ = idx_list.copy()\n",
    "        random.shuffle(idx_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Define indices for labelled, unlabelled training images, and test images\n",
    "# --------------------------------------------------------------------------------\n",
    "def get_pascal_idx(root, train=True, label_num=5):\n",
    "    root = os.path.expanduser(root)\n",
    "    if train:\n",
    "        file_name = root + '/train_aug.txt'\n",
    "    else:\n",
    "        file_name = root + '/val.txt'\n",
    "    with open(file_name) as f:\n",
    "        idx_list = f.read().splitlines()\n",
    "\n",
    "    if train:\n",
    "        labeled_idx = []\n",
    "        save_idx = []\n",
    "        idx_list_ = idx_list.copy()\n",
    "        random.shuffle(idx_list_)\n",
    "        label_counter = np.zeros(21)\n",
    "        label_fill = np.arange(21)\n",
    "        while len(labeled_idx) < label_num:\n",
    "            if len(idx_list_) > 0:\n",
    "                idx = idx_list_.pop()\n",
    "            else:\n",
    "                idx_list_ = save_idx.copy()\n",
    "                idx = idx_list_.pop()\n",
    "                save_idx = []\n",
    "            mask = np.array(Image.open(root + '/SegmentationClassAug/{}.png'.format(idx)))\n",
    "            mask_unique = np.unique(mask)[:-1] if 255 in mask else np.unique(mask)  # remove void class\n",
    "            unique_num = len(mask_unique)   # number of unique classes\n",
    "\n",
    "            # sample image if it includes the lowest appeared class and with more than 3 distinctive classes\n",
    "            if len(labeled_idx) == 0 and unique_num >= 3:\n",
    "                labeled_idx.append(idx)\n",
    "                label_counter[mask_unique] += 1\n",
    "            elif np.any(np.in1d(label_fill, mask_unique)) and unique_num >= 3:\n",
    "                labeled_idx.append(idx)\n",
    "                label_counter[mask_unique] += 1\n",
    "            else:\n",
    "                save_idx.append(idx)\n",
    "\n",
    "            # record any segmentation index with lowest appearance\n",
    "            label_fill = np.where(label_counter == label_counter.min())[0]\n",
    "\n",
    "        return labeled_idx, [idx for idx in idx_list if idx not in labeled_idx]\n",
    "    else:\n",
    "        return idx_list"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "metadata": {},
   "outputs": [],

   "source": [
    "root = \"C:\\\\Users\\\\Admin\\\\OneDrive\\\\C Documents\\\\GitHub\\\\reco\\\\dataset\\\\pascal\"\n",
    "get_pascal_idx(root, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pascal_label_colormap():\n",
    "  \"\"\"Creates a label colormap used in Pascal segmentation benchmark.\n",
    "  Returns:\n",
    "    A colormap for visualizing segmentation results.\n",
    "  \"\"\"\n",
    "  colormap = 255 * np.ones((256, 3), dtype=np.uint8)\n",
    "  colormap[0] = [0, 0, 0]\n",
    "  colormap[1] = [128, 0, 0]\n",
    "  colormap[2] = [0, 128, 0]\n",
    "  colormap[3] = [128, 128, 0]\n",
    "  colormap[4] = [0, 0, 128]\n",
    "  colormap[5] = [128, 0, 128]\n",
    "  colormap[6] = [0, 128, 128]\n",
    "  colormap[7] = [128, 128, 128]\n",
    "  colormap[8] = [64, 0, 0]\n",
    "  colormap[9] = [192, 0, 0]\n",
    "  colormap[10] = [64, 128, 0]\n",
    "  colormap[11] = [192, 128, 0]\n",
    "  colormap[12] = [64, 0, 128]\n",
    "  colormap[13] = [192, 0, 128]\n",
    "  colormap[14] = [64, 128, 128]\n",
    "  colormap[15] = [192, 128, 128]\n",
    "  colormap[16] = [0, 64, 0]\n",
    "  colormap[17] = [128, 64, 0]\n",
    "  colormap[18] = [0, 192, 0]\n",
    "  colormap[19] = [128, 192, 0]\n",
    "  colormap[20] = [0, 64, 128]\n",
    "  return colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildDataLoader:\n",
    "    def __init__(self, dataset, num_labels):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.data_path = 'C:/Users/Admin/OneDrive/C Documents/GitHub/reco/dataset/pascal'\n",
    "        self.im_size = [513, 513]\n",
    "        self.crop_size = [321, 321]\n",
    "        self.num_segments = 21\n",
    "        self.scale_size = (0.5, 1.5)\n",
    "        self.batch_size = 10\n",
    "        self.train_l_idx, self.train_u_idx = get_pascal_idx(self.data_path, train=True, label_num=num_labels)\n",
    "        self.test_idx = get_pascal_idx(self.data_path, train=False)\n",
    "\n",
    "        if num_labels == 0:  # using all data\n",
    "            self.train_l_idx = self.train_u_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Define data augmentation\n",
    "# --------------------------------------------------------------------------------\n",
    "def transform(image, label, logits=None, crop_size=(512, 512), scale_size=(0.8, 1.0), augmentation=True):\n",
    "    # Random rescale image\n",
    "    raw_w, raw_h = image.size\n",
    "    scale_ratio = random.uniform(scale_size[0], scale_size[1])\n",
    "\n",
    "    resized_size = (int(raw_h * scale_ratio), int(raw_w * scale_ratio))\n",
    "    image = transforms_f.resize(image, resized_size, Image.BILINEAR)\n",
    "    label = transforms_f.resize(label, resized_size, Image.NEAREST)\n",
    "    if logits is not None:\n",
    "        logits = transforms_f.resize(logits, resized_size, Image.NEAREST)\n",
    "\n",
    "    # Add padding if rescaled image size is less than crop size\n",
    "    if crop_size == -1:  # use original im size without crop or padding\n",
    "        crop_size = (raw_h, raw_w)\n",
    "\n",
    "    if crop_size[0] > resized_size[0] or crop_size[1] > resized_size[1]:\n",
    "        right_pad, bottom_pad = max(crop_size[1] - resized_size[1], 0), max(crop_size[0] - resized_size[0], 0)\n",
    "        image = transforms_f.pad(image, padding=(0, 0, right_pad, bottom_pad), padding_mode='reflect')\n",
    "        label = transforms_f.pad(label, padding=(0, 0, right_pad, bottom_pad), fill=255, padding_mode='constant')\n",
    "        if logits is not None:\n",
    "            logits = transforms_f.pad(logits, padding=(0, 0, right_pad, bottom_pad), fill=0, padding_mode='constant')\n",
    "\n",
    "    # Random Cropping\n",
    "    i, j, h, w = transforms.RandomCrop.get_params(image, output_size=crop_size)\n",
    "    image = transforms_f.crop(image, i, j, h, w)\n",
    "    label = transforms_f.crop(label, i, j, h, w)\n",
    "    if logits is not None:\n",
    "        logits = transforms_f.crop(logits, i, j, h, w)\n",
    "\n",
    "    if augmentation:\n",
    "        # Random color jitter\n",
    "        if torch.rand(1) > 0.2:\n",
    "            #  color_transform = transforms.ColorJitter((0.75, 1.25), (0.75, 1.25), (0.75, 1.25), (-0.25, 0.25))  For PyTorch 1.9/TorchVision 0.10 users\n",
    "            # color_transform = transforms.ColorJitter.get_params((0.75, 1.25), (0.75, 1.25), (0.75, 1.25), (-0.25, 0.25))\n",
    "            fn_idx, brightness_factor, contrast_factor, saturation_factor, hue_factor = \\\n",
    "              transforms.ColorJitter.get_params((0.75, 1.25), (0.75, 1.25), (0.75, 1.25), (-0.25, 0.25))\n",
    "\n",
    "            for fn_id in fn_idx:\n",
    "              if fn_id == 0 and brightness_factor is not None:\n",
    "                  image = transforms_f.adjust_brightness(image, brightness_factor)\n",
    "              elif fn_id == 1 and contrast_factor is not None:\n",
    "                  image = transforms_f.adjust_contrast(image, contrast_factor)\n",
    "              elif fn_id == 2 and saturation_factor is not None:\n",
    "                  image = transforms_f.adjust_saturation(image, saturation_factor)\n",
    "              elif fn_id == 3 and hue_factor is not None:\n",
    "                  image = transforms_f.adjust_hue(image, hue_factor)\n",
    "            #image = color_transform(image)\n",
    "\n",
    "        # Random Gaussian filter\n",
    "        if torch.rand(1) > 0.5:\n",
    "            sigma = random.uniform(0.15, 1.15)\n",
    "            image = image.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
    "\n",
    "        # Random horizontal flipping\n",
    "        if torch.rand(1) > 0.5:\n",
    "            image = transforms_f.hflip(image)\n",
    "            label = transforms_f.hflip(label)\n",
    "            if logits is not None:\n",
    "                logits = transforms_f.hflip(logits)\n",
    "\n",
    "    # Transform to tensor\n",
    "    image = transforms_f.to_tensor(image)\n",
    "    label = (transforms_f.to_tensor(label) * 255).long()\n",
    "    label[label == 255] = -1  # invalid pixels are re-mapped to index -1\n",
    "    if logits is not None:\n",
    "        logits = transforms_f.to_tensor(logits)\n",
    "\n",
    "    # Apply (ImageNet) normalisation\n",
    "    image = transforms_f.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    if logits is not None:\n",
    "        return image, label, logits\n",
    "    else:\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def batch_transform(data, label, logits, crop_size, scale_size, apply_augmentation):\n",
    "    data_list, label_list, logits_list = [], [], []\n",
    "    device = data.device\n",
    "\n",
    "    for k in range(data.shape[0]):\n",
    "        data_pil, label_pil, logits_pil = tensor_to_pil(data[k], label[k], logits[k])\n",
    "        aug_data, aug_label, aug_logits = transform(data_pil, label_pil, logits_pil,\n",
    "                                                    crop_size=crop_size,\n",
    "                                                    scale_size=scale_size,\n",
    "                                                    augmentation=apply_augmentation)\n",
    "        data_list.append(aug_data.unsqueeze(0))\n",
    "        label_list.append(aug_label)\n",
    "        logits_list.append(aug_logits)\n",
    "\n",
    "    data_trans, label_trans, logits_trans = \\\n",
    "        torch.cat(data_list).to(device), torch.cat(label_list).to(device), torch.cat(logits_list).to(device)\n",
    "    return data_trans, label_trans, logits_trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildDataset(Dataset):\n",
    "    def __init__(self, root, dataset, idx_list, crop_size=(512, 512), scale_size=(0.5, 2.0),\n",
    "                 augmentation=True, train=True, apply_partial=None, partial_seed=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.train = train\n",
    "        self.crop_size = crop_size\n",
    "        self.augmentation = augmentation\n",
    "        self.dataset = dataset\n",
    "        self.idx_list = idx_list\n",
    "        self.scale_size = scale_size\n",
    "        self.apply_partial = apply_partial\n",
    "        self.partial_seed = partial_seed\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_root = Image.open(self.root + '/JPEGImages/{}.jpg'.format(self.idx_list[index]))\n",
    "        if self.apply_partial is None:\n",
    "            label_root = Image.open(self.root + '/SegmentationClassAug/{}.png'.format(self.idx_list[index]))\n",
    "        else:\n",
    "            label_root = Image.open(self.root + '/SegmentationClassAug_{}_{}/{}.png'.format(self.apply_partial,  self.partial_seed, self.idx_list[index],))\n",
    "\n",
    "        image, label = transform(image_root, label_root, None, self.crop_size, self.scale_size, self.augmentation)\n",
    "        return image, label.squeeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "DATA_DIR = ''\n",
    "data_loader = BuildDataLoader:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dced41d100df253a5ef2e3ce847c68478ee9ee572ed365625b465dcce1a06afe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sslseg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
